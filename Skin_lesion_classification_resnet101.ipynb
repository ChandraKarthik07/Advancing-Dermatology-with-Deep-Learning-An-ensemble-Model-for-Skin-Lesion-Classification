{"cells":[{"cell_type":"markdown","metadata":{},"source":[" ### First, import all libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-18T08:17:13.433425Z","iopub.status.busy":"2024-02-18T08:17:13.432974Z","iopub.status.idle":"2024-02-18T08:17:13.449178Z","shell.execute_reply":"2024-02-18T08:17:13.448262Z","shell.execute_reply.started":"2024-02-18T08:17:13.433395Z"},"trusted":true},"outputs":[],"source":["%matplotlib inline\n","# python libraties\n","import os, cv2,itertools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from glob import glob\n","from PIL import Image\n","\n","# pytorch libraries\n","import torch\n","from torch import optim,nn\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader,Dataset\n","from torchvision import models,transforms\n","\n","# sklearn libraries\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","# to make the results are reproducible\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed(10)\n","\n","print(os.listdir(\"../input\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1. Data analysis and preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["Get the all image data pathsï¼Œ match the row information in HAM10000_metadata.csv with its corresponding image"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2024-02-18T08:17:13.451595Z","iopub.status.busy":"2024-02-18T08:17:13.451144Z","iopub.status.idle":"2024-02-18T08:17:13.595831Z","shell.execute_reply":"2024-02-18T08:17:13.595049Z","shell.execute_reply.started":"2024-02-18T08:17:13.451552Z"},"trusted":true},"outputs":[],"source":["data_dir = '../input'\n","all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n","imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n","lesion_type_dict = {\n","    'nv': 'Melanocytic nevi',\n","    'mel': 'dermatofibroma',\n","    'bkl': 'Benign keratosis-like lesions ',\n","    'bcc': 'Basal cell carcinoma',\n","    'akiec': 'Actinic keratoses',\n","    'vasc': 'Vascular lesions',\n","    'df': 'Dermatofibroma'\n","}"]},{"cell_type":"markdown","metadata":{},"source":["This function is used to compute the mean and standard deviation on the whole dataset, will use for inputs normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:17:13.597155Z","iopub.status.busy":"2024-02-18T08:17:13.596876Z","iopub.status.idle":"2024-02-18T08:17:13.607600Z","shell.execute_reply":"2024-02-18T08:17:13.606711Z","shell.execute_reply.started":"2024-02-18T08:17:13.597132Z"},"trusted":true},"outputs":[],"source":["def compute_img_mean_std(image_paths):\n","    img_h, img_w = 224, 224\n","    imgs = []\n","    means, stdevs = [], []\n","\n","    for i in tqdm(range(len(image_paths))):\n","        img = cv2.imread(image_paths[i])\n","        img = cv2.resize(img, (img_h, img_w))\n","        imgs.append(img)\n","\n","    imgs = np.stack(imgs, axis=3)\n","    print(imgs.shape)\n","\n","    imgs = imgs.astype(np.float32) / 255.\n","\n","    for i in range(3):\n","        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n","        means.append(np.mean(pixels))\n","        stdevs.append(np.std(pixels))\n","\n","    means.reverse()  # BGR --> RGB\n","    stdevs.reverse()\n","\n","    print(\"normMean = {}\".format(means))\n","    print(\"normStd = {}\".format(stdevs))\n","    return means,stdevs"]},{"cell_type":"markdown","metadata":{},"source":["Return the mean and std of RGB channels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:17:13.608801Z","iopub.status.busy":"2024-02-18T08:17:13.608569Z","iopub.status.idle":"2024-02-18T08:21:40.144421Z","shell.execute_reply":"2024-02-18T08:21:40.143475Z","shell.execute_reply.started":"2024-02-18T08:17:13.608780Z"},"trusted":true},"outputs":[],"source":["norm_mean,norm_std = compute_img_mean_std(all_image_path)"]},{"cell_type":"markdown","metadata":{},"source":["Add three columns to the original DataFrame, path (image path), cell_type (the whole name),cell_type_idx (the corresponding index  of cell type, as the image label )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:40.147369Z","iopub.status.busy":"2024-02-18T08:21:40.147053Z","iopub.status.idle":"2024-02-18T08:21:40.188685Z","shell.execute_reply":"2024-02-18T08:21:40.187804Z","shell.execute_reply.started":"2024-02-18T08:21:40.147345Z"},"trusted":true},"outputs":[],"source":["df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n","df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n","df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n","df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n","df_original.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Images associated with each lesion_id and filtering them"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:40.190126Z","iopub.status.busy":"2024-02-18T08:21:40.189825Z","iopub.status.idle":"2024-02-18T08:21:40.233140Z","shell.execute_reply":"2024-02-18T08:21:40.232142Z","shell.execute_reply.started":"2024-02-18T08:21:40.190101Z"},"trusted":true},"outputs":[],"source":["df_undup = df_original.groupby('lesion_id').count()\n","df_undup = df_undup[df_undup['image_id'] == 1]\n","df_undup.reset_index(inplace=True)\n","df_undup.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Identifying lesion_id's that have duplicate images and those that have only one image."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:40.234772Z","iopub.status.busy":"2024-02-18T08:21:40.234375Z","iopub.status.idle":"2024-02-18T08:21:47.407149Z","shell.execute_reply":"2024-02-18T08:21:47.406044Z","shell.execute_reply.started":"2024-02-18T08:21:40.234744Z"},"trusted":true},"outputs":[],"source":["def get_duplicates(x):\n","    unique_list = list(df_undup['lesion_id'])\n","    if x in unique_list:\n","        return 'unduplicated'\n","    else:\n","        return 'duplicated'\n","\n","# create a new colum that is a copy of the lesion_id column\n","df_original['duplicates'] = df_original['lesion_id']\n","# apply the function to this new column\n","df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n","df_original.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:47.409101Z","iopub.status.busy":"2024-02-18T08:21:47.408710Z","iopub.status.idle":"2024-02-18T08:21:47.418391Z","shell.execute_reply":"2024-02-18T08:21:47.417476Z","shell.execute_reply.started":"2024-02-18T08:21:47.409058Z"},"trusted":true},"outputs":[],"source":["df_original['duplicates'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### filter out images that don't have duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:47.420351Z","iopub.status.busy":"2024-02-18T08:21:47.419898Z","iopub.status.idle":"2024-02-18T08:21:47.431933Z","shell.execute_reply":"2024-02-18T08:21:47.430961Z","shell.execute_reply.started":"2024-02-18T08:21:47.420317Z"},"trusted":true},"outputs":[],"source":["df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n","df_undup.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:47.433458Z","iopub.status.busy":"2024-02-18T08:21:47.433139Z","iopub.status.idle":"2024-02-18T08:21:47.447592Z","shell.execute_reply":"2024-02-18T08:21:47.446631Z","shell.execute_reply.started":"2024-02-18T08:21:47.433427Z"},"trusted":true},"outputs":[],"source":["# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\n","y = df_undup['cell_type_idx']\n","_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n","df_val.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:47.449113Z","iopub.status.busy":"2024-02-18T08:21:47.448818Z","iopub.status.idle":"2024-02-18T08:21:47.456395Z","shell.execute_reply":"2024-02-18T08:21:47.455427Z","shell.execute_reply.started":"2024-02-18T08:21:47.449087Z"},"trusted":true},"outputs":[],"source":["df_val['cell_type_idx'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:47.457843Z","iopub.status.busy":"2024-02-18T08:21:47.457546Z","iopub.status.idle":"2024-02-18T08:21:49.110137Z","shell.execute_reply":"2024-02-18T08:21:49.109131Z","shell.execute_reply.started":"2024-02-18T08:21:47.457819Z"},"trusted":true},"outputs":[],"source":["# This set will be df_original excluding all rows that are in the val set\n","# This function identifies if an image is part of the train or val set.\n","def get_val_rows(x):\n","    val_list = list(df_val['image_id'])\n","    if str(x) in val_list:\n","        return 'val'\n","    else:\n","        return 'train'\n","\n","# identify train and val rows\n","# create a new colum that is a copy of the image_id column\n","df_original['train_or_val'] = df_original['image_id']\n","# apply the function to this new column\n","df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n","# filter out train rows\n","df_train = df_original[df_original['train_or_val'] == 'train']\n","print(len(df_train))\n","print(len(df_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:49.111576Z","iopub.status.busy":"2024-02-18T08:21:49.111321Z","iopub.status.idle":"2024-02-18T08:21:49.119256Z","shell.execute_reply":"2024-02-18T08:21:49.118243Z","shell.execute_reply.started":"2024-02-18T08:21:49.111555Z"},"trusted":true},"outputs":[],"source":["df_train['cell_type_idx'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:49.124030Z","iopub.status.busy":"2024-02-18T08:21:49.123763Z","iopub.status.idle":"2024-02-18T08:21:49.132178Z","shell.execute_reply":"2024-02-18T08:21:49.131259Z","shell.execute_reply.started":"2024-02-18T08:21:49.124007Z"},"trusted":true},"outputs":[],"source":["df_val['cell_type'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["**As we can see from the statistics of each category above, the training data has a significant class imbalance. I believe there are two places we can start when solving this issue: equalization sampling and loss functions like focused loss that can be utilized to reduce category imbalance during training.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:49.133625Z","iopub.status.busy":"2024-02-18T08:21:49.133282Z","iopub.status.idle":"2024-02-18T08:21:49.181114Z","shell.execute_reply":"2024-02-18T08:21:49.180142Z","shell.execute_reply.started":"2024-02-18T08:21:49.133591Z"},"trusted":true},"outputs":[],"source":["# Assuming df_train is a pandas DataFrame\n","data_aug_rate = [15, 10, 5, 50, 0, 40, 5]\n","for i in range(7):\n","    if data_aug_rate[i]:\n","        subset_df = df_train[df_train['cell_type_idx'] == i]\n","        df_train = pd.concat([df_train, pd.concat([subset_df] * (data_aug_rate[i] - 1))],\n","                            ignore_index=True)\n","\n","df_train['cell_type'].value_counts()\n"]},{"cell_type":"markdown","metadata":{},"source":["At the beginning, I divided the data into three parts, training set, validation set and test set. Considering the small amount of data, I did not further divide the validation set data in practice."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:49.183191Z","iopub.status.busy":"2024-02-18T08:21:49.182518Z","iopub.status.idle":"2024-02-18T08:21:49.198580Z","shell.execute_reply":"2024-02-18T08:21:49.197768Z","shell.execute_reply.started":"2024-02-18T08:21:49.183158Z"},"trusted":true},"outputs":[],"source":["df_train = df_train.reset_index()\n","df_val = df_val.reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2. Model building"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:49.199973Z","iopub.status.busy":"2024-02-18T08:21:49.199647Z","iopub.status.idle":"2024-02-18T08:21:49.205220Z","shell.execute_reply":"2024-02-18T08:21:49.204250Z","shell.execute_reply.started":"2024-02-18T08:21:49.199947Z"},"trusted":true},"outputs":[],"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:49.206795Z","iopub.status.busy":"2024-02-18T08:21:49.206476Z","iopub.status.idle":"2024-02-18T08:21:49.217349Z","shell.execute_reply":"2024-02-18T08:21:49.216606Z","shell.execute_reply.started":"2024-02-18T08:21:49.206763Z"},"trusted":true},"outputs":[],"source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18, resnet34, resnet50, resnet101\n","        \"\"\"\n","        model_ft = models.resnet101(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","    return model_ft, input_size"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:49.219029Z","iopub.status.busy":"2024-02-18T08:21:49.218694Z","iopub.status.idle":"2024-02-18T08:21:50.250343Z","shell.execute_reply":"2024-02-18T08:21:50.249458Z","shell.execute_reply.started":"2024-02-18T08:21:49.218998Z"},"trusted":true},"outputs":[],"source":["model_name = 'resnet'\n","num_classes = 7\n","feature_extract = False\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","# Define the device:\n","device = torch.device('cuda:0')\n","# Put the model on the device:\n","model = model_ft.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:50.252282Z","iopub.status.busy":"2024-02-18T08:21:50.251893Z","iopub.status.idle":"2024-02-18T08:21:50.259340Z","shell.execute_reply":"2024-02-18T08:21:50.258354Z","shell.execute_reply.started":"2024-02-18T08:21:50.252249Z"},"trusted":true},"outputs":[],"source":["# define the transformation of the train images.\n","train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n","                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n","                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n","                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n","# define the transformation of the val images.\n","val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n","                                    transforms.Normalize(norm_mean, norm_std)])"]},{"cell_type":"markdown","metadata":{},"source":["### Define a pytorch dataloader for this dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:50.260995Z","iopub.status.busy":"2024-02-18T08:21:50.260656Z","iopub.status.idle":"2024-02-18T08:21:50.269922Z","shell.execute_reply":"2024-02-18T08:21:50.268900Z","shell.execute_reply.started":"2024-02-18T08:21:50.260963Z"},"trusted":true},"outputs":[],"source":["\n","class HAM10000(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        # Load data and get label\n","        X = Image.open(self.df['path'][index])\n","        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n","\n","        if self.transform:\n","            X = self.transform(X)\n","\n","        return X, y"]},{"cell_type":"markdown","metadata":{},"source":["### Define the training set and validation set using the table train_df and using our defined transitions (train_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:50.271633Z","iopub.status.busy":"2024-02-18T08:21:50.271194Z","iopub.status.idle":"2024-02-18T08:21:50.280426Z","shell.execute_reply":"2024-02-18T08:21:50.279586Z","shell.execute_reply.started":"2024-02-18T08:21:50.271606Z"},"trusted":true},"outputs":[],"source":["\n","training_set = HAM10000(df_train, transform=train_transform)\n","train_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n","\n","validation_set = HAM10000(df_val, transform=train_transform)\n","val_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)"]},{"cell_type":"markdown","metadata":{},"source":["### Adam optimizer, use cross entropy loss as our loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:50.281939Z","iopub.status.busy":"2024-02-18T08:21:50.281612Z","iopub.status.idle":"2024-02-18T08:21:50.294916Z","shell.execute_reply":"2024-02-18T08:21:50.294109Z","shell.execute_reply.started":"2024-02-18T08:21:50.281913Z"},"trusted":true},"outputs":[],"source":["optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3. Model training"]},{"cell_type":"markdown","metadata":{},"source":["This function is used during training process, to calculation the loss and accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:50.296896Z","iopub.status.busy":"2024-02-18T08:21:50.296304Z","iopub.status.idle":"2024-02-18T08:21:50.304260Z","shell.execute_reply":"2024-02-18T08:21:50.303434Z","shell.execute_reply.started":"2024-02-18T08:21:50.296863Z"},"trusted":true},"outputs":[],"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:50.305549Z","iopub.status.busy":"2024-02-18T08:21:50.305267Z","iopub.status.idle":"2024-02-18T08:21:50.316941Z","shell.execute_reply":"2024-02-18T08:21:50.316144Z","shell.execute_reply.started":"2024-02-18T08:21:50.305526Z"},"trusted":true},"outputs":[],"source":["total_loss_train, total_acc_train = [],[]\n","def train(train_loader, model, criterion, optimizer, epoch):\n","    model.train()\n","    train_loss = AverageMeter()\n","    train_acc = AverageMeter()\n","    curr_iter = (epoch - 1) * len(train_loader)\n","    for i, data in enumerate(train_loader):\n","        images, labels = data\n","        N = images.size(0)\n","        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n","        images = Variable(images).to(device)\n","        labels = Variable(labels).to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        prediction = outputs.max(1, keepdim=True)[1]\n","        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n","        train_loss.update(loss.item())\n","        curr_iter += 1\n","        if (i + 1) % 100 == 0:\n","            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n","                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n","            total_loss_train.append(train_loss.avg)\n","            total_acc_train.append(train_acc.avg)\n","    return train_loss.avg, train_acc.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:50.318330Z","iopub.status.busy":"2024-02-18T08:21:50.318037Z","iopub.status.idle":"2024-02-18T08:21:50.329743Z","shell.execute_reply":"2024-02-18T08:21:50.328973Z","shell.execute_reply.started":"2024-02-18T08:21:50.318308Z"},"trusted":true},"outputs":[],"source":["def validate(val_loader, model, criterion, optimizer, epoch):\n","    model.eval()\n","    val_loss = AverageMeter()\n","    val_acc = AverageMeter()\n","    with torch.no_grad():\n","        for i, data in enumerate(val_loader):\n","            images, labels = data\n","            N = images.size(0)\n","            images = Variable(images).to(device)\n","            labels = Variable(labels).to(device)\n","\n","            outputs = model(images)\n","            prediction = outputs.max(1, keepdim=True)[1]\n","\n","            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n","\n","            val_loss.update(criterion(outputs, labels).item())\n","\n","    print('------------------------------------------------------------')\n","    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n","    print('------------------------------------------------------------')\n","    return val_loss.avg, val_acc.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T08:21:50.331193Z","iopub.status.busy":"2024-02-18T08:21:50.330926Z","iopub.status.idle":"2024-02-18T09:03:53.702943Z","shell.execute_reply":"2024-02-18T09:03:53.701760Z","shell.execute_reply.started":"2024-02-18T08:21:50.331171Z"},"trusted":true},"outputs":[],"source":["epoch_num = 10\n","best_val_acc = 0\n","total_loss_val, total_acc_val = [],[]\n","for epoch in range(1, epoch_num+1):\n","    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n","    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n","    total_loss_val.append(loss_val)\n","    total_acc_val.append(acc_val)\n","    if acc_val > best_val_acc:\n","        best_val_acc = acc_val\n","        print('*****************************************************')\n","        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n","        print('*****************************************************')"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4. Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T09:03:53.704868Z","iopub.status.busy":"2024-02-18T09:03:53.704513Z","iopub.status.idle":"2024-02-18T09:03:54.046273Z","shell.execute_reply":"2024-02-18T09:03:54.045274Z","shell.execute_reply.started":"2024-02-18T09:03:53.704837Z"},"trusted":true},"outputs":[],"source":["fig = plt.figure(num = 2)\n","fig1 = fig.add_subplot(2,1,1)\n","fig2 = fig.add_subplot(2,1,2)\n","fig1.plot(total_loss_train, label = 'training loss')\n","fig1.plot(total_acc_train, label = 'training accuracy')\n","fig2.plot(total_loss_val, label = 'validation loss')\n","fig2.plot(total_acc_val, label = 'validation accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["This function is used during training process, to calculation the loss and accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T09:03:54.047765Z","iopub.status.busy":"2024-02-18T09:03:54.047485Z","iopub.status.idle":"2024-02-18T09:03:54.056667Z","shell.execute_reply":"2024-02-18T09:03:54.055553Z","shell.execute_reply.started":"2024-02-18T09:03:54.047742Z"},"trusted":true},"outputs":[],"source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n"," \n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T09:03:54.058337Z","iopub.status.busy":"2024-02-18T09:03:54.058055Z","iopub.status.idle":"2024-02-18T09:04:01.873574Z","shell.execute_reply":"2024-02-18T09:04:01.872501Z","shell.execute_reply.started":"2024-02-18T09:03:54.058315Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","y_label = []\n","y_predict = []\n","with torch.no_grad():\n","    for i, data in enumerate(val_loader):\n","        images, labels = data\n","        N = images.size(0)\n","        images = Variable(images).to(device)\n","        outputs = model(images)\n","        prediction = outputs.max(1, keepdim=True)[1]\n","        y_label.extend(labels.cpu().numpy())\n","        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n","\n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(y_label, y_predict)\n","# plot the confusion matrix\n","plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n","plot_confusion_matrix(confusion_mtx, plot_labels)"]},{"cell_type":"markdown","metadata":{},"source":["### Generate a classification report"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T09:04:01.875301Z","iopub.status.busy":"2024-02-18T09:04:01.874976Z","iopub.status.idle":"2024-02-18T09:04:01.892479Z","shell.execute_reply":"2024-02-18T09:04:01.891334Z","shell.execute_reply.started":"2024-02-18T09:04:01.875270Z"},"trusted":true},"outputs":[],"source":["\n","report = classification_report(y_label, y_predict, target_names=plot_labels)\n","print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T09:04:01.894466Z","iopub.status.busy":"2024-02-18T09:04:01.893802Z","iopub.status.idle":"2024-02-18T09:04:02.126063Z","shell.execute_reply":"2024-02-18T09:04:02.125035Z","shell.execute_reply.started":"2024-02-18T09:04:01.894433Z"},"trusted":true},"outputs":[],"source":["label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n","plt.bar(np.arange(7),label_frac_error)\n","plt.xlabel('True Label')\n","plt.ylabel('Fraction classified incorrectly')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T09:04:38.554233Z","iopub.status.busy":"2024-02-18T09:04:38.553295Z","iopub.status.idle":"2024-02-18T09:04:42.261956Z","shell.execute_reply":"2024-02-18T09:04:42.261078Z","shell.execute_reply.started":"2024-02-18T09:04:38.554175Z"},"trusted":true},"outputs":[],"source":["model_scripted = torch.jit.script(model) # Export to TorchScript\n","model_scripted.save('resnet.pt') # Save"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":54339,"sourceId":104884,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
